{
 "metadata": {
  "name": "",
  "signature": "sha256:18776614c678d84d1b9ffbd08bffb92c579fd9f2f9fa9f83a489666d0fd682bf"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Neural Network Test"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Ok, so the idea here is to create nodes connected by autocorrecting weights. Let's create a simple version."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from random import randint \n",
      "\n",
      "net = []\n",
      "layer1 = 2\n",
      "layer2 = 10\n",
      "layer3 = 2\n",
      "\n",
      "\n",
      "#Each neuron has an int. If the int is > 1, the neuron fires.\n",
      "#Each neuron has a weight connecting it to the next layer.\n",
      "#These weights can be negative.\n",
      "#Let's start by creating the neurons.\n",
      "\n",
      "for i in xrange(0,3):\n",
      "    net.append([])\n",
      "\n",
      "for i in xrange(0,layer1):\n",
      "    net[0].append(0)\n",
      "for i in xrange(0, layer2):\n",
      "    net[1].append(0)\n",
      "for i in xrange(0, layer3):\n",
      "    net[2].append(0)\n",
      "    \n",
      "#Now let's add the weights\n",
      "\n",
      "weights = []\n",
      "for i in xrange(0,len(net)-1):\n",
      "    weights.append([])\n",
      "    for j in xrange(0, len(net[i])):\n",
      "        weights[i].append([])\n",
      "        for k in xrange(0, len(net[i+1])):\n",
      "            weights[i][j].append(randint(-10,10))\n",
      "\n",
      "\n",
      "# Ok, sweet. Now let's create an array of expected inputs and outputs.\n",
      "# If the two are the same, then the first one fires. Otherwise the second one fires.\n",
      "\n",
      "training = [[[2,2],[1,1]],\n",
      "            [[0,1],[1,1]],\n",
      "            [[1,0],[1,1]],\n",
      "            [[1,1],[1,1]]\n",
      "            ]\n",
      "\n",
      "#Now let's run through the training set\n",
      "\n",
      "def rundata(trainingset):\n",
      "    net[0] = trainingset[0]\n",
      "    for i in xrange(0,len(net)-1):\n",
      "        for j in xrange(0, len(net[i])):\n",
      "            for k in xrange(0,len(weights[i][j])):\n",
      "                if (net[i][j] > 0):\n",
      "                    net[i+1][k] += weights[i][j][k]\n",
      "    for i in xrange(0,len(net[2])):\n",
      "        if (net[2][i] > 0):\n",
      "            net[2][i] = 1\n",
      "        else:\n",
      "            net[2][i] = 0\n",
      "    return net[2]\n",
      "        \n",
      "#Now, correct errors by shifting weights if an end neuron is incorrect.\n",
      "        \n",
      "def correctError(training):\n",
      "    errors = []\n",
      "    for trainingset in training:\n",
      "        result = rundata(trainingset)\n",
      "        layer2errors = [0] * len(net[1])\n",
      "        for i in xrange(0,len(result)):\n",
      "            error = result[i] - trainingset[0][i]\n",
      "            for j in xrange(0, len(weights[1])):\n",
      "                weights[1][j][i] -= error\n",
      "                layer2errors[j] += error\n",
      "            for j in xrange(0,len(layer2errors)):\n",
      "                weights[0][i][j] += layer2errors[j]\n",
      "            errors.append(error)\n",
      "    print errors\n",
      "\n",
      "for i in xrange(0, 10):\n",
      "    correctError(training)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[-1, -2, 1, -1, 0, 1, 0, 0]\n",
        "[-1, -1, 1, -1, 0, 1, 0, 0]\n",
        "[-1, -1, 1, -1, 0, 1, 0, 0]\n",
        "[-1, -2, 1, 0, 0, 1, 0, 0]\n",
        "[-1, -1, 1, 0, 0, 1, 0, 0]\n",
        "[-1, -1, 1, 0, 0, 1, 0, 0]\n",
        "[-1, -1, 1, 0, 0, 1, 0, 0]\n",
        "[-1, -1, 1, 0, 0, 1, 0, 0]\n",
        "[-1, -1, 1, 0, 0, 1, 0, 0]\n",
        "[-1, -1, 1, 0, 0, 1, 0, 0]\n"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Unclear if I'm on the mark, let's start with something that already works and dissect it:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from scipy import optimize\n",
      "from __future__ import division"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class NN_1HL(object):\n",
      "    #Ok, so the network itself is a class.\n",
      "    \n",
      "    def __init__(self, reg_lambda=0, epsilon_init=0.12, hidden_layer_size=25, opti_method='TNC', maxiter=500):\n",
      "        self.reg_lambda = reg_lambda\n",
      "        #Lambda and epsilon vars, one of them seems to be involved in Init. Maybe the learning constant/something to do\n",
      "        #with initial weights?\n",
      "        self.epsilon_init = epsilon_init\n",
      "        #Hidden layer size: clear enouhg.\n",
      "        self.hidden_layer_size = hidden_layer_size\n",
      "        #Sigmoid error functions\n",
      "        self.activation_func = self.sigmoid\n",
      "        self.activation_func_prime = self.sigmoid_prime\n",
      "        #Optimization function, I guess this can chanfe.\n",
      "        self.method = opti_method\n",
      "        #What's a maxiter?\n",
      "        #Got it. Max Iter= maxiterations. The maximum number of iterations in an epoch.\n",
      "        self.maxiter = maxiter\n",
      "    \n",
      "    def sigmoid(self, z):\n",
      "        #Standard sigmoid.\n",
      "        return 1 / (1 + np.exp(-z))\n",
      "    \n",
      "    def sigmoid_prime(self, z):\n",
      "        #Derivative of the sigmoid function, I guess used to calculate the maximum error delta.\n",
      "        sig = self.sigmoid(z)\n",
      "        return sig * (1 - sig)\n",
      "    \n",
      "    def sumsqr(self, a):\n",
      "        #Looks like this just squares the number, why the sum?\n",
      "        #Probably because A is an array of numbers. Gotcha.\n",
      "        return np.sum(a ** 2)\n",
      "    \n",
      "    def rand_init(self, l_in, l_out):\n",
      "        #Used for defining initial weights. It looks like they are in the range +/- epsilon\n",
      "        return np.random.rand(l_out, l_in + 1) * 2 * self.epsilon_init - self.epsilon_init\n",
      "    \n",
      "    def pack_thetas(self, t1, t2):\n",
      "        #Ok, so there are two arrays being passed. They are reshaped (maybe pivoted?) and then concatinated.\n",
      "        #Not clear what's going on here. \n",
      "        return np.concatenate((t1.reshape(-1), t2.reshape(-1)))\n",
      "    \n",
      "    def unpack_thetas(self, thetas, input_layer_size, hidden_layer_size, num_labels):\n",
      "        #Looks like this is the equivalent of inputlayererror, the feedback errors travelling back.\n",
      "        t1_start = 0\n",
      "        t1_end = hidden_layer_size * (input_layer_size + 1)\n",
      "        t1 = thetas[t1_start:t1_end].reshape((hidden_layer_size, input_layer_size + 1))\n",
      "        t2 = thetas[t1_end:].reshape((num_labels, hidden_layer_size + 1))\n",
      "        return t1, t2\n",
      "    \n",
      "    def _forward(self, X, t1, t2):\n",
      "        #Sweetm ok. This sends a signal forward through the network (self).\n",
      "        m = X.shape[0]\n",
      "        ones = None\n",
      "        #Create M (length of X) arrays with a single 1 in them. \n",
      "        if len(X.shape) == 1:\n",
      "            ones = np.array(1).reshape(1,)\n",
      "        else:\n",
      "            ones = np.ones(m).reshape(m,1)\n",
      "        \n",
      "        # Input layer\n",
      "        # Now add a second entry to each of the arrays, X. Why two values?\n",
      "        # These must be the input values.\n",
      "        a1 = np.hstack((ones, X))\n",
      "        \n",
      "        # Hidden Layer\n",
      "        # ok, .t is \"translate\", kind of a pivot.\n",
      "        # .dot is a dot product.\n",
      "        # This is multiplying the weights (t1) by the input values (a1) and getting a resulting matrix (z2)\n",
      "        z2 = np.dot(t1, a1.T)\n",
      "        #Return the sigmoid value of this matrix. \n",
      "        #I'm assuming that applies the function to each value of the matrix?\n",
      "        #This is determining whether or not a given neuron \"activates\" based on the weighted input values. \n",
      "        a2 = self.activation_func(z2)\n",
      "        #Ok, now create a matrix of the sigmoid funtion, with each column being the same\n",
      "        #Now create a matrix of the activated values to apply to the output function.\n",
      "        a2 = np.hstack((ones, a2.T))\n",
      "        print a2\n",
      "        \n",
      "        # Output layer\n",
      "        # Rinse and repeat, need to internalize more matrix multiplication/understand what it's good for.\n",
      "        # Dot product of  t2 and the translated a2 (that is the weights from the final layer and the translated weights from the \n",
      "        # hidden layer.)\n",
      "        z3 = np.dot(t2, a2.T)\n",
      "        #Sigmoid that bad boy.\n",
      "        a3 = self.activation_func(z3)\n",
      "        #Return the input and activation values for each layer like a boss.\n",
      "        return a1, z2, a2, z3, a3\n",
      "    \n",
      "    #Descriptive. I'm guessing this is runs an epoch?\n",
      "    def function(self, thetas, input_layer_size, hidden_layer_size, num_labels, X, y, reg_lambda):\n",
      "        #Make the weights (thetas.)\n",
      "        t1, t2 = self.unpack_thetas(thetas, input_layer_size, hidden_layer_size, num_labels)\n",
      "        \n",
      "        #Arrange the inputs.\n",
      "        m = X.shape[0]\n",
      "        #Ok, create an array of length num_labels, all zeroes, with a 1 at y.\n",
      "        #Probably for matrix multiplication.\n",
      "        Y = np.eye(num_labels)[y]\n",
      "        \n",
      "        #Run forward, ignore everything but a3, which you mark as h.\n",
      "        _, _, _, _, h = self._forward(X, t1, t2)\n",
      "        #Ok, so now we're using multiplication to put H as a row in an array of num_labels?\n",
      "        costPositive = -Y * np.log(h).T\n",
      "        print \"costPositive\"\n",
      "        print costPositive\n",
      "        costNegative = (1 - Y) * np.log(1 - h).T\n",
      "        #Now we're doing the same thing, but with 1-Y and 1-h. This creates the inverse. \n",
      "        #Not TOTALLY certain about this, seems like it's to deal with errors that are both above and below the expected value, given that we're using logs?\n",
      "        cost = costPositive - costNegative\n",
      "        #Now sum across the array, got it. J is an expression of error.\n",
      "        J = np.sum(cost) / m\n",
      "        \n",
      "        #If the learning value is not 0!\n",
      "        if reg_lambda != 0:\n",
      "            t1f = t1[:, 1:]\n",
      "            t2f = t2[:, 1:]\n",
      "            reg = (self.reg_lambda / (2 * m)) * (self.sumsqr(t1f) + self.sumsqr(t2f))\n",
      "            #Update J. This seems like it's the \"momentum\" function discussed earlier.\n",
      "            J = J + reg\n",
      "        return J\n",
      "        \n",
      "    #Need to examine. Similar function, but it looks like it returns the derivative of the error function and dives some sort of theta gradient.    \n",
      "    def function_prime(self, thetas, input_layer_size, hidden_layer_size, num_labels, X, y, reg_lambda):\n",
      "        t1, t2 = self.unpack_thetas(thetas, input_layer_size, hidden_layer_size, num_labels)\n",
      "        \n",
      "        m = X.shape[0]\n",
      "        t1f = t1[:, 1:]\n",
      "        t2f = t2[:, 1:]\n",
      "        Y = np.eye(num_labels)[y]\n",
      "        \n",
      "        Delta1, Delta2 = 0, 0\n",
      "        for i, row in enumerate(X):\n",
      "            a1, z2, a2, z3, a3 = self._forward(row, t1, t2)\n",
      "            \n",
      "            # Backprop\n",
      "            d3 = a3 - Y[i, :].T\n",
      "            d2 = np.dot(t2f.T, d3) * self.activation_func_prime(z2)\n",
      "            \n",
      "            Delta2 += np.dot(d3[np.newaxis].T, a2[np.newaxis])\n",
      "            Delta1 += np.dot(d2[np.newaxis].T, a1[np.newaxis])\n",
      "            \n",
      "        Theta1_grad = (1 / m) * Delta1\n",
      "        Theta2_grad = (1 / m) * Delta2\n",
      "        \n",
      "        if reg_lambda != 0:\n",
      "            Theta1_grad[:, 1:] = Theta1_grad[:, 1:] + (reg_lambda / m) * t1f\n",
      "            Theta2_grad[:, 1:] = Theta2_grad[:, 1:] + (reg_lambda / m) * t2f\n",
      "        \n",
      "        return self.pack_thetas(Theta1_grad, Theta2_grad)\n",
      "    \n",
      "    def fit(self, X, y):\n",
      "        num_features = X.shape[0]\n",
      "        input_layer_size = X.shape[1]\n",
      "        num_labels = len(set(y))\n",
      "        \n",
      "        theta1_0 = self.rand_init(input_layer_size, self.hidden_layer_size)\n",
      "        theta2_0 = self.rand_init(self.hidden_layer_size, num_labels)\n",
      "        thetas0 = self.pack_thetas(theta1_0, theta2_0)\n",
      "        \n",
      "        options = {'maxiter': self.maxiter}\n",
      "        _res = optimize.minimize(self.function, thetas0, jac=self.function_prime, method=self.method, \n",
      "                                 args=(input_layer_size, self.hidden_layer_size, num_labels, X, y, 0), options=options)\n",
      "        \n",
      "        self.t1, self.t2 = self.unpack_thetas(_res.x, input_layer_size, self.hidden_layer_size, num_labels)\n",
      "    \n",
      "    def predict(self, X):\n",
      "        return self.predict_proba(X).argmax(0)\n",
      "    \n",
      "    def predict_proba(self, X):\n",
      "        _, _, _, _, h = self._forward(X, self.t1, self.t2)\n",
      "        return h"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 30
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "m = np.ones(6).reshape(3,2)\n",
      "np.dot(m, m.T)\n",
      "print m.T\n",
      "print m\n",
      "print np.dot(m, m.T)\n",
      "print np.eye(10)[3]\n",
      "m = xrange(0, 10)\n",
      "m[:, 1:]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 1.  1.  1.]\n",
        " [ 1.  1.  1.]]\n",
        "[[ 1.  1.]\n",
        " [ 1.  1.]\n",
        " [ 1.  1.]]\n",
        "[[ 2.  2.  2.]\n",
        " [ 2.  2.  2.]\n",
        " [ 2.  2.  2.]]\n",
        "[ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n"
       ]
      },
      {
       "ename": "TypeError",
       "evalue": "sequence index must be integer, not 'tuple'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-78-a615f8d8310e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mprint\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meye\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;31mTypeError\u001b[0m: sequence index must be integer, not 'tuple'"
       ]
      }
     ],
     "prompt_number": 78
    }
   ],
   "metadata": {}
  }
 ]
}